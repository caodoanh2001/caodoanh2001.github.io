---
---

@string{aps = {American Physical Society,}}

@article{clear2025,
  author={Doanh C. Bui and Thinh V. Le and Ba Hung Ngo and Tae Jong Choi},
  journal={Under review}, 
  title={CLEAR: Cross-Transformers with Pre-trained Language Model for Person Attribute Recognition and Retrieval}, 
  volume={},
  pages={},
  year={2025},
  selected={false},
  preview={clear.png},
  bibtex_show={false}
}

@article{eswa2025,
  author={Ba Hung Ngo and Doanh C. Bui and Tae Jong Choi},
  journal={Expert Systems with Applications}, 
  title={How to Enrich Cross-domain Representations? Data Augmentation, Cycle-pseudo Labeling, and Category-aware Graph Learning}, 
  volume={},
  pages={},
  year={2025},
  selected={false},
  pdf={https://www.sciencedirect.com/science/article/pii/S0957417425002192},
  preview={eswa.jpeg},
  bibtex_show={false}
}

@inproceedings{higda2024,
  bibtex_show={false},
  author={Ba Hung Ngo* and Doanh C. Bui* and Nhat-Tuong Do-Tran and Tae Jong Choi},
  booktitle={39th Annual AAAI Conference on Artificial Intelligence (AAAI) (Accepted)},
  title={HiGDA: Hierarchical Graph of Nodes to Learn Local-to-Global Topology for Semi-Supervised Domain Adaptation}, 
  volume={},
  pages={},
  year={2024},
  selected={true},
  preview={higda.jpeg},
  bibtex_show={false},
  pdf={https://arxiv.org/pdf/2412.11819}
}

@inproceedings{bui2024mecformer,
  selected={true},
  author={Doanh C. Bui and Kwak, Jin Tae},
  booktitle={17th Asian Conference on Computer Vision (ACCV) (Accepted)},
  title={MECFormer: Multi-task Whole Slide Image Classification with Expert Consultation Network},
  volume={},
  pages={},
  year={2024},
  selected={true},
  preview={MECFormer.png},
  pdf={https://arxiv.org/abs/2410.04507},
  bibtex_show={false},
  pdf={https://openaccess.thecvf.com/content/ACCV2024/html/Bui_MECFormer_Multi-task_Whole_Slide_Image_Classification_with_Expert_Consultation_Network_ACCV_2024_paper.html}
}

@article{BuiTMDNet2024,
  bibtex_show={true},
  author={Doanh C. Bui and Tam V. Nguyen and Khang Nguyen},
  journal={Machine Vision and Applications}, 
  title={Transformer with Multi-level Grid Features and Depth Pooling for Image Captioning}, 
  year={2024},
  month={Aug},
  day={20},
  volume={35},
  number={5},
  pages={118},
  selected={true},
  preview={TMD.png},
  pdf={https://link.springer.com/article/10.1007/s00138-024-01599-z}
}

@article{scubanet2024,
  bibtex_show={true},
  selected={true},
  preview={scubanet.png},
  author={Bui, Doanh C. and Song, Boram and Kim, Kyungeun and Kwak, Jin Tae},
  journal={IEEE Transactions on Medical Imaging}, 
  title={Spatially-constrained and -unconstrained bi-graph interaction network for multi-organ pathology image classification}, 
  year={2024},
  volume={},
  number={},
  pages={1-1},
  keywords={Pathology;Transformers;Cancer;Image edge detection;Vectors;Computational modeling;Image classification;Computational pathology;cancer classification;graph neural network;Transformer;interaction},
  doi={10.1109/TMI.2024.3436080},
  pdf={https://ieeexplore.ieee.org/document/10616189},
  bibtex_show={false},
}

@InProceedings{10.1007/978-3-031-71626-3_10,
  author="T. L. Vuong, Trinh
  and C. Bui, Doanh
  and Kwak, Jin Tae",
  editor="Bao, Rina
  and Grant, Ellen
  and Kirkpatrick, Andrew
  and Wachs, Juan
  and Ou, Yangming",
  title="QuIIL at T3 Challenge: Towards Automation in Life-Saving Intervention Procedures from First-Person View",
  booktitle="AI for Brain Lesion Detection and Trauma Video Action Recognition",
  year="2025",
  publisher="Springer Nature Switzerland",
  address="Cham",
  pages="82--93",
  abstract="In this paper, we present our solutions for a spectrum of automation tasks in life-saving intervention procedures within the Trauma THOMPSON (T3) Challenge, encompassing action recognition, action anticipation, and Visual Question Answering (VQA). For action recognition and anticipation, we propose a pre-processing strategy that samples and stitches multiple inputs into a single image and then incorporates momentum- and attention-based knowledge distillation to improve the performance of the two tasks. For training, we present an action dictionary-guided design, which consistently yields the most favorable results across our experiments. In the realm of VQA, we leverage object-level features and deploy co-attention networks to train both object and question features. Notably, we introduce a novel frame-question cross-attention mechanism at the network's core for enhanced performance. Our solutions achieve the {\$}{\$}2^{\{}nd{\}}{\$}{\$}rank in action recognition and anticipation tasks and {\$}{\$}1^{\{}st{\}}{\$}{\$}rank in the VQA task. The source code is available at https://github.com/QuIIL/QuIIL{\_}thompson{\_}solution.",
  isbn="978-3-031-71626-3",
  preview={thompson.jpg},
  year={2024},
  pdf={https://arxiv.org/abs/2407.13216},
  bibtex_show={true},
  preview={thompson.jpg},
  pdf={https://link.springer.com/chapter/10.1007/978-3-031-71626-3_10},
  selected={true},
}

@InProceedings{10.1007/978-3-031-72083-3_12,
  author="Bui, Doanh C.
  and Vuong, Trinh Thi Le
  and Kwak, Jin Tae",
  editor="Linguraru, Marius George
  and Dou, Qi
  and Feragen, Aasa
  and Giannarou, Stamatia
  and Glocker, Ben
  and Lekadir, Karim
  and Schnabel, Julia A.",
  title="FALFormer: Feature-Aware Landmarks Self-attention for Whole-Slide Image Classification",
  booktitle="Medical Image Computing and Computer Assisted Intervention -- MICCAI 2024",
  year="2024",
  publisher="Springer Nature Switzerland",
  address="Cham",
  pages="123--132",
  abstract="Slide-level classification for whole-slide images (WSIs) has been widely recognized as a crucial problem in digital and computational pathology. Current approaches commonly consider WSIs as a bag of cropped patches and process them via multiple instance learning due to the large number of patches, which cannot fully explore the relationship among patches; in other words, the global information cannot be fully incorporated into decision making. Herein, we propose an efficient and effective slide-level classification model, named as FALFormer, that can process a WSI as a whole so as to fully exploit the relationship among the entire patches and to improve the classification performance. FALFormer is built based upon Transformers and self-attention mechanism. To lessen the computational burden of the original self-attention mechanism and to process the entire patches together in a WSI, FALFormer employs Nystr{\"o}m self-attention which approximates the computation by using a smaller number of tokens or landmarks. For effective learning, FALFormer introduces feature-aware landmarks to enhance the representation power of the landmarks and the quality of the approximation. We systematically evaluate the performance of FALFormer using two public datasets, including CAMELYON16 and TCGA-BRCA. The experimental results demonstrate that FALFormer achieves superior performance on both datasets, outperforming the state-of-the-art methods for the slide-level classification. This suggests that FALFormer can facilitate an accurate and precise analysis of WSIs, potentially leading to improved diagnosis and prognosis on WSIs.",
  isbn="978-3-031-72083-3",
  bibtex_show={true},
  pdf={https://link.springer.com/chapter/10.1007/978-3-031-72083-3_12},
  preview={miccai2024.png},
}

@inproceedings{bui2024efficient,
  title={Efficient semantic segmentation for computational pathology},
  author={Doanh C. Bui and Kim, Changsu and Kwak, Jin Tae},
  booktitle={Medical Imaging 2024: Digital and Computational Pathology},
  volume={12933},
  pages={145--151},
  year={2024},
  preview={spie.png},
  organization={SPIE},
}

@article{BUI2024108112,
  bibtex_show={true},
  title = {DAX-Net: A dual-branch dual-task adaptive cross-weight feature fusion network for robust multi-class cancer classification in pathology images},
  journal = {Computer Methods and Programs in Biomedicine},
  volume = {248},
  pages = {108112},
  year = {2024},
  issn = {0169-2607},
  doi = {https://doi.org/10.1016/j.cmpb.2024.108112},
  url = {https://www.sciencedirect.com/science/article/pii/S0169260724001081},
  selected={true},
  preview={daxnet.png},
  author = {Doanh C. Bui and Boram Song and Kyungeun Kim and Jin Tae Kwak},
  keywords = {Cancer classification, Hybrid model, CNN, Transformer, Feature fusion, Multi-task learning},
  pdf = {https://www.sciencedirect.com/science/article/pii/S0169260724001081}
}

@article{10466765,
  bibtex_show={true},
  author={Tran, Tung Minh and Bui, Doanh C. and Nguyen, Tam V. and Nguyen, Khang},
  journal={IEEE Transactions on Circuits and Systems for Video Technology}, 
  title={Transformer-based Spatio-Temporal Unsupervised Traffic Anomaly Detection in Aerial Videos}, 
  year={2024},
  volume={},
  number={},
  pages={1-1},
  selected={true},
  preview={astt.png},
  keywords={Surveillance;Anomaly detection;Transformers;Traffic control;Computational modeling;Training;Task analysis;Anomaly Detection;Convolutional Neural Networks;Vision Transformers;Traffic Surveillance;Aerial Images},
  doi={10.1109/TCSVT.2024.3376399},
  pdf={https://ieeexplore.ieee.org/abstract/document/10466765}
}

@InProceedings{Bui_2024_WACV,
    bibtex_show={true},
    author    = {Bui, Doanh C. and Le, Thinh V. and Ngo, Ba Hung},
    title     = {C2T-Net: Channel-Aware Cross-Fused Transformer-Style Networks for Pedestrian Attribute Recognition},
    booktitle = {Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) Workshops},
    month     = {January},
    year      = {2024},
    pages     = {351-358},
    selected={true},
    preview={c2tnet.png},
    pdf={https://openaccess.thecvf.com/content/WACV2024W/RWS/html/Bui_C2T-Net_Channel-Aware_Cross-Fused_Transformer-Style_Networks_for_Pedestrian_Attribute_Recognition_WACVW_2024_paper.html}
}

@article{nguyen2023improving,
  bibtex_show={true},
  title={Improving human--object interaction with auxiliary semantic information and enhanced instance representation},
  author={Nguyen, Khang and Le, Thinh V and Van, Huyen Ngoc N and Bui, Doanh C},
  journal={Pattern Recognition Letters},
  volume={175},
  pages={38--43},
  year={2023},
  selected={true},
  publisher={Elsevier},
  doi={https://doi.org/10.1016/j.patrec.2023.09.013},
  preview={PRL.png},
  pdf={https://www.sciencedirect.com/science/article/pii/S0167865523002611}
}

@article{9924781,
  bibtex_show={true},
  author={Bui, Doanh C. and Nguyen, Nghia Hieu and Vo, Nguyen D. and Thai, Uyen Han Thuy and Nguyen, Khang},
  booktitle={2022 International Conference on Multimedia Analysis and Pattern Recognition (MAPR)}, 
  title={Vi-DRSNet: A Novel Hybrid Model for Vietnamese Image Captioning in Healthcare Domain}, 
  year={2022},
  volume={},
  number={},
  pages={1-6},
  doi={10.1109/MAPR56351.2022.9924781},
  preview={vidrsnet.png}}

@article{9852099,
  bibtex_show={true},
  author={Le, Thinh V. and Van, Huyen Ngoc N. and Bui, Doanh C. and Vo, Phuong and Vo, Nguyen D. and Nguyen, Khang},
  booktitle={2022 IEEE Ninth International Conference on Communications and Electronics (ICCE)}, 
  title={Empirical Study of RepPoints Representation for Object Detection in Aerial Images}, 
  year={2022},
  volume={},
  number={},
  pages={337-342},
  doi={10.1109/ICCE55644.2022.9852099},
  preview={Reppoint1.png}}

@article{9732974,
  bibtex_show={true},
  author={Nguyen, Khang and Bui, Doanh C. and Trinh, Truc and Vo, Nguyen D.},
  journal={IEEE Access}, 
  title={EAES: Effective Augmented Embedding Spaces for Text-Based Image Captioning}, 
  year={2022},
  volume={10},
  number={},
  pages={32443-32452},
  doi={10.1109/ACCESS.2022.3158763},
  preview={ic2.jpg}}

@article{Nguyen2022,
  bibtex_show={true},
  title = {Analysis of the Influence of De-hazing Methods on Vehicle Detection in Aerial Images},
  journal = {International Journal of Advanced Computer Science and Applications},
  doi = {10.14569/IJACSA.2022.01306100},
  url = {http://dx.doi.org/10.14569/IJACSA.2022.01306100},
  year = {2022},
  publisher = {The Science and Information Organization},
  volume = {13},
  number = {6},
  author = {Khang Nguyen and Phuc Nguyen and Doanh C. Bui and Minh Tran and Nguyen D. Vo},
  preview={04_visualize_hazy.png}
}

@article{9642128,
  bibtex_show={true},
  author={Bui, Doanh C. and Truong, Dung and Vo, Nguyen D. and Nguyen, Khang},
  booktitle={2021 RIVF International Conference on Computing and Communication Technologies (RIVF)}, 
  title={MC-OCR Challenge 2021: Deep Learning Approach for Vietnamese Receipts OCR}, 
  year={2021},
  volume={},
  number={},
  pages={1-6},
  doi={10.1109/RIVF51545.2021.9642128},
  preview={ReceiptsOCR.png}}
